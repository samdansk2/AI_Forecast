{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2432ab",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis & Visualization - AI/ML Market Analysis\n",
    "\n",
    "This notebook provides comprehensive visual analysis and insights from the AI market datasets using advanced visualizations.\n",
    "\n",
    "## Objectives:\n",
    "- Perform detailed correlation analysis\n",
    "- Identify trends and patterns over time\n",
    "- Analyze regional patterns and differences\n",
    "- Conduct statistical tests and hypothesis validation\n",
    "- Create interactive visualizations for exploration\n",
    "- Generate business insights and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf591dc0",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80ae160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comprehensive visualization libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Utilities\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Load processed datasets\n",
    "processed_dir = Path('../data/processed')\n",
    "market_df = pd.read_csv(processed_dir / 'ai_market_engineered.csv')\n",
    "popularity_df = pd.read_csv(processed_dir / 'ai_popularity_clean.csv')\n",
    "\n",
    "print(\"✅ Data loaded for EDA!\")\n",
    "print(f\"Market data (engineered): {market_df.shape}\")\n",
    "print(f\"Popularity data: {popularity_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875dc4b7",
   "metadata": {},
   "source": [
    "## 2. Market Evolution Timeline Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a0344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive market evolution timeline\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    subplot_titles=(\n",
    "        'AI Revenue Growth (2018-2025)',\n",
    "        'Market Value Explosion', \n",
    "        'Adoption Rate Progression',\n",
    "        'Job Market Transformation',\n",
    "        'Organizational AI Integration',\n",
    "        'Technology Impact Metrics'\n",
    "    ),\n",
    "    vertical_spacing=0.08,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# 1. AI Revenue Growth\n",
    "if 'ai_software_revenue_in_billions' in market_df.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=market_df['year'],\n",
    "            y=market_df['ai_software_revenue_in_billions'],\n",
    "            mode='lines+markers',\n",
    "            name='AI Revenue',\n",
    "            line=dict(color='#1f77b4', width=3),\n",
    "            marker=dict(size=8)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# 2. Market Value\n",
    "if 'global_ai_market_value_in_billions' in market_df.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=market_df['year'],\n",
    "            y=market_df['global_ai_market_value_in_billions'],\n",
    "            mode='lines+markers',\n",
    "            name='Market Value',\n",
    "            line=dict(color='#ff7f0e', width=3),\n",
    "            marker=dict(size=8)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# 3. Adoption Rate\n",
    "if 'ai_adoption' in market_df.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=market_df['year'],\n",
    "            y=market_df['ai_adoption'],\n",
    "            mode='lines+markers',\n",
    "            name='Adoption Rate',\n",
    "            line=dict(color='#2ca02c', width=3),\n",
    "            marker=dict(size=8)\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# 4. Job Impact\n",
    "if 'estimated_jobs_eliminated_by_ai_millions' in market_df.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=market_df['year'],\n",
    "            y=market_df['estimated_jobs_eliminated_by_ai_millions'],\n",
    "            mode='lines+markers',\n",
    "            name='Jobs Eliminated',\n",
    "            line=dict(color='#d62728', width=3)\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "if 'estimated_new_jobs_created_by_ai_millions' in market_df.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=market_df['year'],\n",
    "            y=market_df['estimated_new_jobs_created_by_ai_millions'],\n",
    "            mode='lines+markers',\n",
    "            name='Jobs Created',\n",
    "            line=dict(color='#9467bd', width=3)\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "# 5. Organizational Metrics\n",
    "if 'organizations_using_ai' in market_df.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=market_df['year'],\n",
    "            y=market_df['organizations_using_ai'],\n",
    "            mode='lines+markers',\n",
    "            name='Orgs Using AI',\n",
    "            line=dict(color='#8c564b', width=3)\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "\n",
    "# 6. Technology Impact\n",
    "if 'americans_using_voice_assistants' in market_df.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=market_df['year'],\n",
    "            y=market_df['americans_using_voice_assistants'],\n",
    "            mode='lines+markers',\n",
    "            name='Voice Assistant Usage',\n",
    "            line=dict(color='#e377c2', width=3)\n",
    "        ),\n",
    "        row=3, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=900,\n",
    "    title_text=\"AI Market Evolution Timeline (2018-2025)\",\n",
    "    title_x=0.5,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b310072f",
   "metadata": {},
   "source": [
    "## 3. Advanced Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c268b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced correlation analysis\n",
    "def advanced_correlation_analysis(df, method='pearson'):\n",
    "    \"\"\"\n",
    "    Perform comprehensive correlation analysis\n",
    "    \"\"\"\n",
    "    # Select numeric columns\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    if method == 'pearson':\n",
    "        corr_matrix = numeric_df.corr(method='pearson')\n",
    "    elif method == 'spearman':\n",
    "        corr_matrix = numeric_df.corr(method='spearman')\n",
    "    else:\n",
    "        corr_matrix = numeric_df.corr(method='kendall')\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "# Calculate different correlation matrices\n",
    "corr_pearson = advanced_correlation_analysis(market_df, 'pearson')\n",
    "corr_spearman = advanced_correlation_analysis(market_df, 'spearman')\n",
    "\n",
    "# Create interactive correlation heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=corr_pearson.values,\n",
    "    x=corr_pearson.columns,\n",
    "    y=corr_pearson.columns,\n",
    "    colorscale='RdBu',\n",
    "    zmid=0,\n",
    "    text=np.round(corr_pearson.values, 2),\n",
    "    texttemplate=\"%{text}\",\n",
    "    textfont={\"size\": 8},\n",
    "    hoverongaps=False\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='AI Market Data - Correlation Matrix (Pearson)',\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    xaxis_tickangle=-45\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Find and display strongest correlations\n",
    "def find_strong_correlations(corr_matrix, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Find correlations above threshold\n",
    "    \"\"\"\n",
    "    strong_corr = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > threshold and not np.isnan(corr_val):\n",
    "                strong_corr.append({\n",
    "                    'Variable_1': corr_matrix.columns[i],\n",
    "                    'Variable_2': corr_matrix.columns[j],\n",
    "                    'Correlation': corr_val,\n",
    "                    'Abs_Correlation': abs(corr_val)\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(strong_corr).sort_values('Abs_Correlation', ascending=False)\n",
    "\n",
    "strong_correlations = find_strong_correlations(corr_pearson, threshold=0.7)\n",
    "\n",
    "print(\"🔗 STRONG CORRELATIONS (|r| > 0.7):\")\n",
    "print(\"=\" * 60)\n",
    "if not strong_correlations.empty:\n",
    "    for idx, row in strong_correlations.head(10).iterrows():\n",
    "        var1 = row['Variable_1'][:30] + '...' if len(row['Variable_1']) > 30 else row['Variable_1']\n",
    "        var2 = row['Variable_2'][:30] + '...' if len(row['Variable_2']) > 30 else row['Variable_2']\n",
    "        print(f\"   {var1} ↔ {var2}: {row['Correlation']:.3f}\")\n",
    "else:\n",
    "    print(\"   No strong correlations found above threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0a81a4",
   "metadata": {},
   "source": [
    "## 4. Time Series Decomposition and Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720990fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series decomposition for key metrics\n",
    "def analyze_time_series_components(df, target_col, year_col='year'):\n",
    "    \"\"\"\n",
    "    Decompose time series into trend, seasonal, and residual components\n",
    "    \"\"\"\n",
    "    if target_col not in df.columns or year_col not in df.columns:\n",
    "        print(f\"❌ Columns not found: {target_col} or {year_col}\")\n",
    "        return None\n",
    "    \n",
    "    # Create time series\n",
    "    ts_data = df[[year_col, target_col]].dropna()\n",
    "    ts_data = ts_data.set_index(year_col)[target_col]\n",
    "    \n",
    "    if len(ts_data) < 4:\n",
    "        print(f\"⚠️ Not enough data points for decomposition: {len(ts_data)}\")\n",
    "        return None\n",
    "    \n",
    "    # Perform decomposition (use additive for trend analysis)\n",
    "    try:\n",
    "        decomposition = seasonal_decompose(ts_data, model='additive', period=min(4, len(ts_data)//2))\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "        \n",
    "        # Original data\n",
    "        decomposition.observed.plot(ax=axes[0], title=f'Original - {target_col.replace(\"_\", \" \").title()}')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Trend\n",
    "        decomposition.trend.plot(ax=axes[1], title='Trend Component', color='red')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Seasonal\n",
    "        decomposition.seasonal.plot(ax=axes[2], title='Seasonal Component', color='green')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Residual\n",
    "        decomposition.resid.plot(ax=axes[3], title='Residual Component', color='purple')\n",
    "        axes[3].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle(f'Time Series Decomposition: {target_col.replace(\"_\", \" \").title()}', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return decomposition\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Decomposition failed for {target_col}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Analyze key time series\n",
    "key_metrics = [\n",
    "    'ai_software_revenue_in_billions',\n",
    "    'global_ai_market_value_in_billions',\n",
    "    'ai_adoption'\n",
    "]\n",
    "\n",
    "decompositions = {}\n",
    "for metric in key_metrics:\n",
    "    if metric in market_df.columns:\n",
    "        print(f\"\\n📈 Analyzing {metric.replace('_', ' ').title()}...\")\n",
    "        decomp = analyze_time_series_components(market_df, metric)\n",
    "        if decomp is not None:\n",
    "            decompositions[metric] = decomp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca3609e",
   "metadata": {},
   "source": [
    "## 5. Growth Rate and Momentum Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive growth analysis\n",
    "growth_cols = [col for col in market_df.columns if 'growth' in col]\n",
    "\n",
    "if growth_cols:\n",
    "    # Create growth comparison chart\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    colors = px.colors.qualitative.Set3\n",
    "    \n",
    "    for i, col in enumerate(growth_cols[:6]):  # Show top 6 growth metrics\n",
    "        if not market_df[col].isna().all():\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=market_df['year'],\n",
    "                    y=market_df[col],\n",
    "                    mode='lines+markers',\n",
    "                    name=col.replace('_', ' ').title(),\n",
    "                    line=dict(color=colors[i % len(colors)], width=2)\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Growth Rates Comparison Over Time',\n",
    "        xaxis_title='Year',\n",
    "        yaxis_title='Growth Rate (%)',\n",
    "        height=600,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Growth statistics\n",
    "    print(\"📊 GROWTH RATE STATISTICS:\")\n",
    "    print(\"=\" * 50)\n",
    "    for col in growth_cols[:5]:\n",
    "        if not market_df[col].isna().all():\n",
    "            mean_growth = market_df[col].mean()\n",
    "            std_growth = market_df[col].std()\n",
    "            max_growth = market_df[col].max()\n",
    "            min_growth = market_df[col].min()\n",
    "            \n",
    "            col_display = col.replace('_', ' ').title()[:40]\n",
    "            print(f\"📈 {col_display}:\")\n",
    "            print(f\"   Mean: {mean_growth:.1f}%, Std: {std_growth:.1f}%\")\n",
    "            print(f\"   Range: [{min_growth:.1f}%, {max_growth:.1f}%]\")\n",
    "else:\n",
    "    print(\"⚠️ No growth features found in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a831cb0",
   "metadata": {},
   "source": [
    "## 6. Regional Analysis and Global Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze global AI popularity patterns\n",
    "def analyze_regional_patterns(df):\n",
    "    \"\"\"\n",
    "    Analyze regional patterns in AI popularity\n",
    "    \"\"\"\n",
    "    print(\"🌍 REGIONAL AI POPULARITY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Countries with highest AI interest\n",
    "    country_col = None\n",
    "    popularity_col = None\n",
    "    \n",
    "    # Find relevant columns\n",
    "    for col in df.columns:\n",
    "        if 'country' in col.lower():\n",
    "            country_col = col\n",
    "        elif 'popularity' in col.lower() and df[col].dtype in ['int64', 'float64']:\n",
    "            popularity_col = col\n",
    "            break\n",
    "    \n",
    "    if country_col and popularity_col:\n",
    "        # Clean data for analysis\n",
    "        regional_data = df[[country_col, popularity_col]].dropna()\n",
    "        regional_data = regional_data[regional_data[country_col] != '']\n",
    "        \n",
    "        if len(regional_data) > 0:\n",
    "            # Top countries by AI popularity\n",
    "            top_countries = regional_data.nlargest(15, popularity_col)\n",
    "            \n",
    "            # Create bar chart\n",
    "            fig = px.bar(\n",
    "                top_countries,\n",
    "                x=popularity_col,\n",
    "                y=country_col,\n",
    "                orientation='h',\n",
    "                title='Top 15 Countries by AI Popularity',\n",
    "                labels={popularity_col: 'AI Popularity Score', country_col: 'Country'},\n",
    "                color=popularity_col,\n",
    "                color_continuous_scale='viridis'\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(height=600, yaxis={'categoryorder': 'total ascending'})\n",
    "            fig.show()\n",
    "            \n",
    "            print(f\"\\n🏆 TOP 10 COUNTRIES BY AI POPULARITY:\")\n",
    "            for i, (idx, row) in enumerate(top_countries.head(10).iterrows(), 1):\n",
    "                country = row[country_col]\n",
    "                score = row[popularity_col]\n",
    "                print(f\"{i:2d}. {country}: {score}\")\n",
    "            \n",
    "            return regional_data\n",
    "    \n",
    "    print(\"⚠️ Could not find country and popularity columns for regional analysis\")\n",
    "    return None\n",
    "\n",
    "# Perform regional analysis\n",
    "regional_data = analyze_regional_patterns(popularity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28716801",
   "metadata": {},
   "source": [
    "## 7. Statistical Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb4e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical hypothesis testing\n",
    "def perform_hypothesis_tests(df):\n",
    "    \"\"\"\n",
    "    Perform various statistical tests on the data\n",
    "    \"\"\"\n",
    "    print(\"🧪 STATISTICAL HYPOTHESIS TESTING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Test 1: Is there a significant trend in AI revenue?\n",
    "    if 'ai_software_revenue_in_billions' in df.columns and 'year' in df.columns:\n",
    "        revenue_data = df[['year', 'ai_software_revenue_in_billions']].dropna()\n",
    "        if len(revenue_data) > 3:\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "                revenue_data['year'], revenue_data['ai_software_revenue_in_billions']\n",
    "            )\n",
    "            \n",
    "            print(f\"📈 AI Revenue Trend Test:\")\n",
    "            print(f\"   Slope: {slope:.2f} billion/year\")\n",
    "            print(f\"   R²: {r_value**2:.3f}\")\n",
    "            print(f\"   P-value: {p_value:.6f}\")\n",
    "            print(f\"   Significant trend: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "            \n",
    "            results['revenue_trend'] = {\n",
    "                'slope': slope,\n",
    "                'r_squared': r_value**2,\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05\n",
    "            }\n",
    "    \n",
    "    # Test 2: Correlation between adoption and revenue\n",
    "    if 'ai_adoption' in df.columns and 'ai_software_revenue_in_billions' in df.columns:\n",
    "        adoption_revenue = df[['ai_adoption', 'ai_software_revenue_in_billions']].dropna()\n",
    "        if len(adoption_revenue) > 3:\n",
    "            corr_coef, p_value = pearsonr(\n",
    "                adoption_revenue['ai_adoption'], \n",
    "                adoption_revenue['ai_software_revenue_in_billions']\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n🔗 Adoption-Revenue Correlation Test:\")\n",
    "            print(f\"   Correlation: {corr_coef:.3f}\")\n",
    "            print(f\"   P-value: {p_value:.6f}\")\n",
    "            print(f\"   Significant correlation: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "            \n",
    "            results['adoption_revenue_corr'] = {\n",
    "                'correlation': corr_coef,\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05\n",
    "            }\n",
    "    \n",
    "    # Test 3: Job creation vs elimination balance\n",
    "    if 'estimated_new_jobs_created_by_ai_millions' in df.columns and 'estimated_jobs_eliminated_by_ai_millions' in df.columns:\n",
    "        jobs_data = df[['estimated_new_jobs_created_by_ai_millions', 'estimated_jobs_eliminated_by_ai_millions']].dropna()\n",
    "        if len(jobs_data) > 3:\n",
    "            # Paired t-test\n",
    "            t_stat, p_value = stats.ttest_rel(\n",
    "                jobs_data['estimated_new_jobs_created_by_ai_millions'],\n",
    "                jobs_data['estimated_jobs_eliminated_by_ai_millions']\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n👥 Job Creation vs Elimination Test:\")\n",
    "            print(f\"   T-statistic: {t_stat:.3f}\")\n",
    "            print(f\"   P-value: {p_value:.6f}\")\n",
    "            print(f\"   Job creation > elimination: {'Yes' if t_stat > 0 and p_value < 0.05 else 'Inconclusive'}\")\n",
    "            \n",
    "            results['job_balance_test'] = {\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'jobs_creation_higher': t_stat > 0 and p_value < 0.05\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Perform hypothesis tests\n",
    "test_results = perform_hypothesis_tests(market_df)\n",
    "\n",
    "print(f\"\\n📋 Statistical tests completed: {len(test_results)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbca2602",
   "metadata": {},
   "source": [
    "## 8. Interactive Visualization Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40dd286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive dashboard-style visualization\n",
    "def create_interactive_dashboard(df):\n",
    "    \"\"\"\n",
    "    Create comprehensive interactive dashboard\n",
    "    \"\"\"\n",
    "    # Key metrics cards (simulated)\n",
    "    if 'year' in df.columns:\n",
    "        latest_year = df['year'].max()\n",
    "        latest_data = df[df['year'] == latest_year].iloc[0]\n",
    "        \n",
    "        print(f\"📊 AI MARKET DASHBOARD - {int(latest_year)}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Key metrics display\n",
    "        metrics = {\n",
    "            'AI Software Revenue': ('ai_software_revenue_in_billions', 'B'),\n",
    "            'Market Value': ('global_ai_market_value_in_billions', 'B'),\n",
    "            'Adoption Rate': ('ai_adoption', '%'),\n",
    "            'Organizations Using AI': ('organizations_using_ai', '%'),\n",
    "            'Expected Revenue Increase': ('estimated_revenue_increase_from_ai_trillions_usd', 'T')\n",
    "        }\n",
    "        \n",
    "        for metric_name, (col_name, unit) in metrics.items():\n",
    "            if col_name in df.columns:\n",
    "                value = latest_data[col_name]\n",
    "                if not pd.isna(value):\n",
    "                    if unit == 'T':\n",
    "                        print(f\"💰 {metric_name}: ${value:.1f}{unit}\")\n",
    "                    elif unit == 'B':\n",
    "                        print(f\"💰 {metric_name}: ${value:.1f}{unit}\")\n",
    "                    else:\n",
    "                        print(f\"📊 {metric_name}: {value:.1f}{unit}\")\n",
    "    \n",
    "    # Create multi-metric comparison\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Revenue vs Market Value', 'Adoption Trends', \n",
    "                       'Job Market Impact', 'Growth Momentum'),\n",
    "        specs=[[{\"secondary_y\": True}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Plot 1: Revenue vs Market Value (dual axis)\n",
    "    if 'ai_software_revenue_in_billions' in df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df['year'], y=df['ai_software_revenue_in_billions'],\n",
    "                      mode='lines+markers', name='Software Revenue',\n",
    "                      line=dict(color='blue', width=3)),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    if 'global_ai_market_value_in_billions' in df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df['year'], y=df['global_ai_market_value_in_billions'],\n",
    "                      mode='lines+markers', name='Market Value',\n",
    "                      line=dict(color='red', width=3)),\n",
    "            row=1, col=1, secondary_y=True\n",
    "        )\n",
    "    \n",
    "    # Plot 2: Adoption trends\n",
    "    adoption_cols = [col for col in df.columns if 'adoption' in col.lower() and df[col].dtype in ['int64', 'float64']]\n",
    "    for i, col in enumerate(adoption_cols[:3]):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df['year'], y=df[col],\n",
    "                      mode='lines+markers', name=col.replace('_', ' ').title()),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Plot 3: Job impact\n",
    "    if 'net_job_impact' in df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=df['year'], y=df['net_job_impact'],\n",
    "                  name='Net Job Impact',\n",
    "                  marker_color=['red' if x < 0 else 'green' for x in df['net_job_impact']]),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Plot 4: Growth momentum\n",
    "    if 'ai_software_revenue_in_billions_yoy_growth' in df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df['year'], y=df['ai_software_revenue_in_billions_yoy_growth'],\n",
    "                      mode='lines+markers', name='Revenue Growth Rate',\n",
    "                      line=dict(color='purple', width=3)),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=\"AI Market Interactive Dashboard\")\n",
    "    fig.show()\n",
    "\n",
    "# Create the dashboard\n",
    "create_interactive_dashboard(market_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f09f3",
   "metadata": {},
   "source": [
    "## 9. Market Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc71b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies in market data\n",
    "def detect_market_anomalies(df, target_cols):\n",
    "    \"\"\"\n",
    "    Detect anomalies using statistical methods\n",
    "    \"\"\"\n",
    "    anomalies = {}\n",
    "    \n",
    "    for col in target_cols:\n",
    "        if col in df.columns:\n",
    "            data = df[col].dropna()\n",
    "            if len(data) > 3:\n",
    "                # Z-score method\n",
    "                z_scores = np.abs(stats.zscore(data))\n",
    "                z_anomalies = np.where(z_scores > 2)[0]  # 2 standard deviations\n",
    "                \n",
    "                # IQR method\n",
    "                Q1 = data.quantile(0.25)\n",
    "                Q3 = data.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                iqr_anomalies = np.where((data < lower_bound) | (data > upper_bound))[0]\n",
    "                \n",
    "                anomalies[col] = {\n",
    "                    'z_score_anomalies': z_anomalies.tolist(),\n",
    "                    'iqr_anomalies': iqr_anomalies.tolist(),\n",
    "                    'z_score_threshold': 2,\n",
    "                    'iqr_bounds': (lower_bound, upper_bound)\n",
    "                }\n",
    "                \n",
    "                if len(z_anomalies) > 0 or len(iqr_anomalies) > 0:\n",
    "                    print(f\"\\n⚠️ Anomalies detected in {col.replace('_', ' ').title()}:\")\n",
    "                    if len(z_anomalies) > 0:\n",
    "                        years = df.iloc[z_anomalies]['year'].values if 'year' in df.columns else z_anomalies\n",
    "                        print(f\"   Z-score anomalies: {years}\")\n",
    "                    if len(iqr_anomalies) > 0:\n",
    "                        years = df.iloc[iqr_anomalies]['year'].values if 'year' in df.columns else iqr_anomalies\n",
    "                        print(f\"   IQR anomalies: {years}\")\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "# Detect anomalies in key metrics\n",
    "key_metrics = [\n",
    "    'ai_software_revenue_in_billions',\n",
    "    'global_ai_market_value_in_billions',\n",
    "    'ai_adoption'\n",
    "]\n",
    "\n",
    "anomaly_results = detect_market_anomalies(market_df, key_metrics)\n",
    "\n",
    "if not any(anomaly_results.values()):\n",
    "    print(\"✅ No significant anomalies detected in key metrics\")\n",
    "else:\n",
    "    print(f\"\\n📊 Anomaly detection completed for {len(anomaly_results)} metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7c1670",
   "metadata": {},
   "source": [
    "## 10. Business Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate business insights\n",
    "def generate_business_insights(df, test_results, anomaly_results):\n",
    "    \"\"\"\n",
    "    Generate actionable business insights\n",
    "    \"\"\"\n",
    "    print(\"💡 BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    insights = []\n",
    "    \n",
    "    # Market growth insights\n",
    "    if 'revenue_trend' in test_results:\n",
    "        slope = test_results['revenue_trend']['slope']\n",
    "        r_squared = test_results['revenue_trend']['r_squared']\n",
    "        \n",
    "        if slope > 0 and r_squared > 0.8:\n",
    "            insights.append(f\"🚀 Strong Revenue Growth: AI software revenue is growing at ${slope:.1f}B/year with {r_squared:.1%} consistency\")\n",
    "            insights.append(f\"💼 Investment Opportunity: Predictable growth pattern suggests stable investment returns\")\n",
    "    \n",
    "    # Adoption insights\n",
    "    if 'ai_adoption' in df.columns:\n",
    "        latest_adoption = df['ai_adoption'].iloc[-1]\n",
    "        if latest_adoption > 50:\n",
    "            insights.append(f\"📈 Market Maturity: {latest_adoption:.0f}% adoption indicates mainstream acceptance\")\n",
    "        elif latest_adoption > 30:\n",
    "            insights.append(f\"⚡ Growth Phase: {latest_adoption:.0f}% adoption shows market is in rapid expansion\")\n",
    "        else:\n",
    "            insights.append(f\"🌱 Early Stage: {latest_adoption:.0f}% adoption indicates significant growth potential\")\n",
    "    \n",
    "    # Job market insights\n",
    "    if 'job_balance_test' in test_results:\n",
    "        if test_results['job_balance_test']['jobs_creation_higher']:\n",
    "            insights.append(f\"👥 Positive Job Impact: AI creates more jobs than it eliminates (statistically significant)\")\n",
    "        else:\n",
    "            insights.append(f\"⚖️ Job Market Transition: Mixed impact on employment requires careful monitoring\")\n",
    "    \n",
    "    # Market efficiency insights\n",
    "    if 'market_efficiency_ratio' in df.columns:\n",
    "        avg_efficiency = df['market_efficiency_ratio'].mean()\n",
    "        if avg_efficiency < 0.3:\n",
    "            insights.append(f\"💎 High Growth Potential: Low revenue-to-market ratio ({avg_efficiency:.2f}) suggests unrealized value\")\n",
    "    \n",
    "    # Display insights\n",
    "    for i, insight in enumerate(insights, 1):\n",
    "        print(f\"{i:2d}. {insight}\")\n",
    "    \n",
    "    # Generate recommendations\n",
    "    print(f\"\\n🎯 STRATEGIC RECOMMENDATIONS:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    recommendations = [\n",
    "        \"🏢 For Businesses: Accelerate AI adoption to gain competitive advantage\",\n",
    "        \"💰 For Investors: Consider AI software companies with strong revenue growth\",\n",
    "        \"👨‍💼 For Workforce: Invest in AI-complementary skills and reskilling programs\",\n",
    "        \"🏛️ For Policymakers: Support transition programs for displaced workers\",\n",
    "        \"🔬 For Researchers: Focus on ethical AI and human-AI collaboration\"\n",
    "    ]\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {rec}\")\n",
    "    \n",
    "    return insights, recommendations\n",
    "\n",
    "# Generate insights\n",
    "insights, recommendations = generate_business_insights(market_df, test_results, anomaly_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd908e",
   "metadata": {},
   "source": [
    "## 11. Export Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a41d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save EDA results and insights\n",
    "results_dir = Path('../results')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create comprehensive analysis summary\n",
    "analysis_summary = {\n",
    "    'analysis_timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'dataset_info': {\n",
    "        'market_data_shape': market_df.shape,\n",
    "        'popularity_data_shape': popularity_df.shape,\n",
    "        'total_features_analyzed': len(market_df.columns)\n",
    "    },\n",
    "    'statistical_tests': test_results,\n",
    "    'anomaly_detection': {k: len(v.get('z_score_anomalies', [])) + len(v.get('iqr_anomalies', []))\n",
    "                         for k, v in anomaly_results.items()},\n",
    "    'business_insights': insights,\n",
    "    'recommendations': recommendations\n",
    "}\n",
    "\n",
    "# Save correlation matrices\n",
    "if 'corr_pearson' in locals():\n",
    "    corr_pearson.to_csv(results_dir / 'correlation_matrix_pearson.csv')\n",
    "    print(\"💾 Saved: correlation_matrix_pearson.csv\")\n",
    "\n",
    "if 'corr_spearman' in locals():\n",
    "    corr_spearman.to_csv(results_dir / 'correlation_matrix_spearman.csv')\n",
    "    print(\"💾 Saved: correlation_matrix_spearman.csv\")\n",
    "\n",
    "# Save strong correlations\n",
    "if 'strong_correlations' in locals() and not strong_correlations.empty:\n",
    "    strong_correlations.to_csv(results_dir / 'strong_correlations.csv', index=False)\n",
    "    print(\"💾 Saved: strong_correlations.csv\")\n",
    "\n",
    "# Save regional analysis\n",
    "if 'regional_data' in locals() and regional_data is not None:\n",
    "    regional_data.to_csv(results_dir / 'regional_ai_popularity.csv', index=False)\n",
    "    print(\"💾 Saved: regional_ai_popularity.csv\")\n",
    "\n",
    "# Save insights summary\n",
    "insights_df = pd.DataFrame({\n",
    "    'type': ['insight'] * len(insights) + ['recommendation'] * len(recommendations),\n",
    "    'content': insights + recommendations,\n",
    "    'timestamp': pd.Timestamp.now().isoformat()\n",
    "})\n",
    "insights_df.to_csv(results_dir / 'business_insights.csv', index=False)\n",
    "\n",
    "print(\"💾 Saved: business_insights.csv\")\n",
    "print(f\"\\n📊 EDA Analysis Summary:\")\n",
    "print(f\"   ✅ {len(insights)} business insights generated\")\n",
    "print(f\"   ✅ {len(recommendations)} strategic recommendations\")\n",
    "print(f\"   ✅ {len(test_results)} statistical tests performed\")\n",
    "print(f\"   ✅ {len(anomaly_results)} metrics analyzed for anomalies\")\n",
    "\n",
    "print(\"\\n🚀 NEXT STEPS:\")\n",
    "print(\"   1. Move to 05_model_development.ipynb for ML modeling\")\n",
    "print(\"   2. Use insights to guide model selection and feature engineering\")\n",
    "print(\"   3. Focus on high-correlation features for predictions\")\n",
    "\n",
    "print(\"\\n✅ EDA & Visualization Phase Complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
