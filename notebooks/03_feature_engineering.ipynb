{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a877a3ff",
   "metadata": {},
   "source": [
    "# Feature Engineering - AI/ML Market Analysis\n",
    "\n",
    "This notebook creates advanced features from the cleaned AI market datasets to enhance predictive modeling capabilities.\n",
    "\n",
    "## Objectives:\n",
    "- Create growth rates and momentum indicators\n",
    "- Generate moving averages and trend features\n",
    "- Build lag features for time series analysis\n",
    "- Create interaction features between variables\n",
    "- Perform feature selection and importance ranking\n",
    "- Prepare features for machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc15b023",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1739e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned datasets loaded successfully!\n",
      "Market data shape: (8, 22)\n",
      "Popularity data shape: (250, 12)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Feature engineering libraries\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_regression, SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load cleaned datasets\n",
    "processed_dir = Path('../data/processed')\n",
    "market_df = pd.read_csv(processed_dir / 'ai_market_clean.csv')\n",
    "popularity_df = pd.read_csv(processed_dir / 'ai_popularity_clean.csv')\n",
    "\n",
    "print(\"✅ Cleaned datasets loaded successfully!\")\n",
    "print(f\"Market data shape: {market_df.shape}\")\n",
    "print(f\"Popularity data shape: {popularity_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a21a4b",
   "metadata": {},
   "source": [
    "## 2. Growth Rate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddbc4595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Growth Features Created:\n",
      "   ✓ ai_software_revenue_in_billions_yoy_growth\n",
      "   ✓ ai_software_revenue_in_billions_cumulative_growth\n",
      "   ✓ ai_software_revenue_in_billions_rolling_2y_growth\n",
      "   ✓ ai_software_revenue_in_billions_growth_acceleration\n",
      "   ✓ global_ai_market_value_in_billions_yoy_growth\n",
      "   ✓ global_ai_market_value_in_billions_cumulative_growth\n",
      "   ✓ global_ai_market_value_in_billions_rolling_2y_growth\n",
      "   ✓ global_ai_market_value_in_billions_growth_acceleration\n",
      "   ✓ organizations_using_ai_yoy_growth\n",
      "   ✓ organizations_using_ai_cumulative_growth\n",
      "   ✓ organizations_using_ai_rolling_2y_growth\n",
      "   ✓ organizations_using_ai_growth_acceleration\n",
      "   ✓ estimated_revenue_increase_from_ai_trillions_usd_yoy_growth\n",
      "   ✓ estimated_revenue_increase_from_ai_trillions_usd_cumulative_growth\n",
      "   ✓ estimated_revenue_increase_from_ai_trillions_usd_rolling_2y_growth\n",
      "   ✓ estimated_revenue_increase_from_ai_trillions_usd_growth_acceleration\n",
      "\n",
      "📊 Total new growth features: 16\n"
     ]
    }
   ],
   "source": [
    "# Create growth rate features\n",
    "def create_growth_features(df, target_cols):\n",
    "    \"\"\"\n",
    "    Create various growth rate indicators\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    for col in target_cols:\n",
    "        if col in df.columns:\n",
    "            # Year-over-year growth rate\n",
    "            df_features[f'{col}_yoy_growth'] = df[col].pct_change() * 100\n",
    "            \n",
    "            # Cumulative growth from base year\n",
    "            base_value = df[col].iloc[0]\n",
    "            df_features[f'{col}_cumulative_growth'] = ((df[col] / base_value) - 1) * 100\n",
    "            \n",
    "            # Rolling growth rate (2-year)\n",
    "            df_features[f'{col}_rolling_2y_growth'] = df[col].rolling(window=2).apply(\n",
    "                lambda x: ((x.iloc[-1] / x.iloc[0]) - 1) * 100 if len(x) == 2 and x.iloc[0] != 0 else np.nan\n",
    "            )\n",
    "            \n",
    "            # Growth acceleration (second derivative)\n",
    "            growth_rate = df[col].pct_change()\n",
    "            df_features[f'{col}_growth_acceleration'] = growth_rate.diff()\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Define key columns for growth analysis\n",
    "growth_columns = [\n",
    "    'ai_software_revenue_in_billions',\n",
    "    'global_ai_market_value_in_billions',\n",
    "    'ai_adoption',\n",
    "    'organizations_using_ai',\n",
    "    'estimated_revenue_increase_from_ai_trillions_usd'\n",
    "]\n",
    "\n",
    "# Create growth features\n",
    "market_with_growth = create_growth_features(market_df, growth_columns)\n",
    "\n",
    "print(\"📈 Growth Features Created:\")\n",
    "new_growth_cols = [col for col in market_with_growth.columns if 'growth' in col]\n",
    "for col in new_growth_cols:\n",
    "    print(f\"   ✓ {col}\")\n",
    "\n",
    "print(f\"\\n📊 Total new growth features: {len(new_growth_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e9d3e",
   "metadata": {},
   "source": [
    "## 3. Moving Averages and Trend Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create moving average and trend features\n",
    "def create_trend_features(df, target_cols, windows=[2, 3, 5]):\n",
    "    \"\"\"\n",
    "    Create moving averages and trend indicators\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    for col in target_cols:\n",
    "        if col in df.columns:\n",
    "            for window in windows:\n",
    "                if window <= len(df):  # Only create if we have enough data points\n",
    "                    # Moving average\n",
    "                    df_features[f'{col}_ma_{window}'] = df[col].rolling(window=window).mean()\n",
    "                    \n",
    "                    # Moving standard deviation\n",
    "                    df_features[f'{col}_std_{window}'] = df[col].rolling(window=window).std()\n",
    "                    \n",
    "                    # Trend direction (above/below moving average)\n",
    "                    ma_col = f'{col}_ma_{window}'\n",
    "                    df_features[f'{col}_above_ma_{window}'] = (df[col] > df_features[ma_col]).astype(int)\n",
    "            \n",
    "            # Linear trend using scipy\n",
    "            x = np.arange(len(df))\n",
    "            y = df[col].values\n",
    "            if not np.isnan(y).all():\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "                df_features[f'{col}_trend_slope'] = slope\n",
    "                df_features[f'{col}_trend_r_squared'] = r_value**2\n",
    "                \n",
    "                # Detrended values\n",
    "                trend_line = slope * x + intercept\n",
    "                df_features[f'{col}_detrended'] = y - trend_line\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Apply trend features\n",
    "market_with_trends = create_trend_features(market_with_growth, growth_columns)\n",
    "\n",
    "print(\"📊 Trend Features Created:\")\n",
    "new_trend_cols = [col for col in market_with_trends.columns if any(x in col for x in ['_ma_', '_std_', '_above_', '_trend_', '_detrended'])]\n",
    "for col in new_trend_cols[:10]:  # Show first 10\n",
    "    print(f\"   ✓ {col}\")\n",
    "\n",
    "print(f\"\\n📊 Total new trend features: {len(new_trend_cols)}\")\n",
    "if len(new_trend_cols) > 10:\n",
    "    print(f\"   (Showing first 10 of {len(new_trend_cols)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cfc18f",
   "metadata": {},
   "source": [
    "## 4. Lag Features for Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb13301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features\n",
    "def create_lag_features(df, target_cols, lags=[1, 2, 3]):\n",
    "    \"\"\"\n",
    "    Create lag features for time series analysis\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    for col in target_cols:\n",
    "        if col in df.columns:\n",
    "            for lag in lags:\n",
    "                if lag < len(df):  # Only create if we have enough data points\n",
    "                    # Basic lag\n",
    "                    df_features[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "                    \n",
    "                    # Lag difference (change from lag period)\n",
    "                    df_features[f'{col}_diff_lag_{lag}'] = df[col] - df[col].shift(lag)\n",
    "                    \n",
    "                    # Lag ratio (current vs lag)\n",
    "                    df_features[f'{col}_ratio_lag_{lag}'] = df[col] / df[col].shift(lag)\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Apply lag features\n",
    "market_with_lags = create_lag_features(market_with_trends, growth_columns)\n",
    "\n",
    "print(\"⏰ Lag Features Created:\")\n",
    "new_lag_cols = [col for col in market_with_lags.columns if 'lag' in col]\n",
    "for col in new_lag_cols[:10]:  # Show first 10\n",
    "    print(f\"   ✓ {col}\")\n",
    "\n",
    "print(f\"\\n📊 Total new lag features: {len(new_lag_cols)}\")\n",
    "if len(new_lag_cols) > 10:\n",
    "    print(f\"   (Showing first 10 of {len(new_lag_cols)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19270b1",
   "metadata": {},
   "source": [
    "## 5. Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd205083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features\n",
    "def create_interaction_features(df):\n",
    "    \"\"\"\n",
    "    Create meaningful interaction features based on domain knowledge\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Revenue per organization using AI\n",
    "    if 'ai_software_revenue_in_billions' in df.columns and 'organizations_using_ai' in df.columns:\n",
    "        df_features['revenue_per_org_using_ai'] = (\n",
    "            df['ai_software_revenue_in_billions'] / (df['organizations_using_ai'] / 100)\n",
    "        )\n",
    "    \n",
    "    # Market efficiency (revenue vs market value ratio)\n",
    "    if 'ai_software_revenue_in_billions' in df.columns and 'global_ai_market_value_in_billions' in df.columns:\n",
    "        df_features['market_efficiency_ratio'] = (\n",
    "            df['ai_software_revenue_in_billions'] / df['global_ai_market_value_in_billions']\n",
    "        )\n",
    "    \n",
    "    # Job displacement ratio\n",
    "    if 'estimated_jobs_eliminated_by_ai_millions' in df.columns and 'estimated_new_jobs_created_by_ai_millions' in df.columns:\n",
    "        df_features['job_creation_ratio'] = (\n",
    "            df['estimated_new_jobs_created_by_ai_millions'] / \n",
    "            df['estimated_jobs_eliminated_by_ai_millions']\n",
    "        )\n",
    "        \n",
    "        df_features['net_job_impact'] = (\n",
    "            df['estimated_new_jobs_created_by_ai_millions'] - \n",
    "            df['estimated_jobs_eliminated_by_ai_millions']\n",
    "        )\n",
    "    \n",
    "    # Adoption momentum (organizations using vs planning)\n",
    "    if 'organizations_using_ai' in df.columns and 'organizations_planning_to_implement_ai' in df.columns:\n",
    "        df_features['adoption_momentum'] = (\n",
    "            df['organizations_planning_to_implement_ai'] - df['organizations_using_ai']\n",
    "        )\n",
    "    \n",
    "    # ROI indicator (revenue increase vs adoption)\n",
    "    if 'estimated_revenue_increase_from_ai_trillions_usd' in df.columns and 'ai_adoption' in df.columns:\n",
    "        df_features['ai_roi_efficiency'] = (\n",
    "            df['estimated_revenue_increase_from_ai_trillions_usd'] / (df['ai_adoption'] / 100)\n",
    "        )\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Apply interaction features\n",
    "market_with_interactions = create_interaction_features(market_with_lags)\n",
    "\n",
    "print(\"🔗 Interaction Features Created:\")\n",
    "interaction_cols = [col for col in market_with_interactions.columns \n",
    "                   if any(x in col for x in ['ratio', 'efficiency', 'momentum', 'impact', 'per_'])]\n",
    "for col in interaction_cols:\n",
    "    print(f\"   ✓ {col}\")\n",
    "\n",
    "print(f\"\\n📊 Total interaction features: {len(interaction_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd465e3",
   "metadata": {},
   "source": [
    "## 6. Technical Indicators and Market Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd4042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create technical indicators similar to financial markets\n",
    "def create_technical_indicators(df, target_cols):\n",
    "    \"\"\"\n",
    "    Create technical indicators for market analysis\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    for col in target_cols:\n",
    "        if col in df.columns:\n",
    "            # RSI-like momentum indicator\n",
    "            delta = df[col].diff()\n",
    "            gain = delta.where(delta > 0, 0).rolling(window=3).mean()\n",
    "            loss = (-delta.where(delta < 0, 0)).rolling(window=3).mean()\n",
    "            rs = gain / loss\n",
    "            df_features[f'{col}_momentum_index'] = 100 - (100 / (1 + rs))\n",
    "            \n",
    "            # Bollinger Bands equivalent\n",
    "            rolling_mean = df[col].rolling(window=3).mean()\n",
    "            rolling_std = df[col].rolling(window=3).std()\n",
    "            df_features[f'{col}_upper_band'] = rolling_mean + (2 * rolling_std)\n",
    "            df_features[f'{col}_lower_band'] = rolling_mean - (2 * rolling_std)\n",
    "            df_features[f'{col}_band_position'] = (\n",
    "                (df[col] - df_features[f'{col}_lower_band']) / \n",
    "                (df_features[f'{col}_upper_band'] - df_features[f'{col}_lower_band'])\n",
    "            )\n",
    "            \n",
    "            # Volatility measure\n",
    "            df_features[f'{col}_volatility'] = df[col].rolling(window=3).std()\n",
    "            \n",
    "            # Z-score (standardized position)\n",
    "            df_features[f'{col}_zscore'] = (df[col] - df[col].mean()) / df[col].std()\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Apply technical indicators\n",
    "market_with_technical = create_technical_indicators(market_with_interactions, growth_columns)\n",
    "\n",
    "print(\"📊 Technical Indicators Created:\")\n",
    "technical_cols = [col for col in market_with_technical.columns \n",
    "                 if any(x in col for x in ['momentum', 'band', 'volatility', 'zscore'])]\n",
    "for col in technical_cols[:10]:  # Show first 10\n",
    "    print(f\"   ✓ {col}\")\n",
    "\n",
    "print(f\"\\n📊 Total technical indicators: {len(technical_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ffdba2",
   "metadata": {},
   "source": [
    "## 7. Seasonal and Cyclical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17423b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create seasonal and cyclical features\n",
    "def create_temporal_features(df, year_col='year'):\n",
    "    \"\"\"\n",
    "    Create temporal and cyclical features\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    if year_col in df.columns:\n",
    "        # Years since start\n",
    "        start_year = df[year_col].min()\n",
    "        df_features['years_since_start'] = df[year_col] - start_year\n",
    "        \n",
    "        # Business cycle indicators (based on year position)\n",
    "        total_years = df[year_col].max() - start_year + 1\n",
    "        df_features['cycle_position'] = (df[year_col] - start_year) / total_years\n",
    "        \n",
    "        # Early/Mid/Late period indicators\n",
    "        df_features['early_period'] = (df_features['cycle_position'] <= 0.33).astype(int)\n",
    "        df_features['mid_period'] = ((df_features['cycle_position'] > 0.33) & \n",
    "                                    (df_features['cycle_position'] <= 0.66)).astype(int)\n",
    "        df_features['late_period'] = (df_features['cycle_position'] > 0.66).astype(int)\n",
    "        \n",
    "        # Sine/Cosine features for cyclical patterns\n",
    "        df_features['year_sin'] = np.sin(2 * np.pi * df_features['years_since_start'] / total_years)\n",
    "        df_features['year_cos'] = np.cos(2 * np.pi * df_features['years_since_start'] / total_years)\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Apply temporal features\n",
    "market_with_temporal = create_temporal_features(market_with_technical)\n",
    "\n",
    "print(\"🕒 Temporal Features Created:\")\n",
    "temporal_cols = [col for col in market_with_temporal.columns \n",
    "                if any(x in col for x in ['since', 'cycle', 'period', 'sin', 'cos'])]\n",
    "for col in temporal_cols:\n",
    "    print(f\"   ✓ {col}\")\n",
    "\n",
    "print(f\"\\n📊 Total temporal features: {len(temporal_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f84aae5",
   "metadata": {},
   "source": [
    "## 8. Domain-Specific Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80cf88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create domain-specific AI market features\n",
    "def create_domain_features(df):\n",
    "    \"\"\"\n",
    "    Create AI market domain-specific features\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Market maturity indicators\n",
    "    if 'ai_adoption' in df.columns:\n",
    "        # Adoption maturity stages\n",
    "        df_features['adoption_stage_early'] = (df['ai_adoption'] < 20).astype(int)\n",
    "        df_features['adoption_stage_growth'] = ((df['ai_adoption'] >= 20) & (df['ai_adoption'] < 50)).astype(int)\n",
    "        df_features['adoption_stage_mature'] = (df['ai_adoption'] >= 50).astype(int)\n",
    "    \n",
    "    # Investment attractiveness score\n",
    "    features_for_score = ['ai_adoption', 'organizations_using_ai', 'companies_prioritizing_ai_in_strategy']\n",
    "    available_features = [f for f in features_for_score if f in df.columns]\n",
    "    \n",
    "    if len(available_features) >= 2:\n",
    "        # Simple composite score\n",
    "        score_data = df[available_features].fillna(0)\n",
    "        df_features['investment_attractiveness'] = score_data.mean(axis=1)\n",
    "    \n",
    "    # Risk indicators\n",
    "    if 'net_job_loss_in_the_us' in df.columns:\n",
    "        df_features['job_risk_level'] = np.where(\n",
    "            df['net_job_loss_in_the_us'] > 5, 'High',\n",
    "            np.where(df['net_job_loss_in_the_us'] > 2, 'Medium', 'Low')\n",
    "        )\n",
    "    \n",
    "    # Market acceleration (second derivative of market value)\n",
    "    if 'global_ai_market_value_in_billions' in df.columns:\n",
    "        market_velocity = df['global_ai_market_value_in_billions'].diff()\n",
    "        df_features['market_acceleration'] = market_velocity.diff()\n",
    "    \n",
    "    # Competition intensity (inverse of market efficiency)\n",
    "    if 'market_efficiency_ratio' in df.columns:\n",
    "        df_features['competition_intensity'] = 1 / df['market_efficiency_ratio']\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Apply domain-specific features\n",
    "market_with_domain = create_domain_features(market_with_temporal)\n",
    "\n",
    "print(\"🎯 Domain-Specific Features Created:\")\n",
    "domain_cols = [col for col in market_with_domain.columns \n",
    "              if any(x in col for x in ['stage', 'attractiveness', 'risk', 'acceleration', 'competition'])]\n",
    "for col in domain_cols:\n",
    "    print(f\"   ✓ {col}\")\n",
    "\n",
    "print(f\"\\n📊 Total domain features: {len(domain_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da1d921",
   "metadata": {},
   "source": [
    "## 9. Feature Importance and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0d0b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "def analyze_feature_importance(df, target_col='ai_software_revenue_in_billions'):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using multiple methods\n",
    "    \"\"\"\n",
    "    # Select numeric features only\n",
    "    numeric_df = df.select_dtypes(include=[np.number]).dropna()\n",
    "    \n",
    "    if target_col not in numeric_df.columns:\n",
    "        print(f\"❌ Target column '{target_col}' not found\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = numeric_df.drop(columns=[target_col, 'year'] if 'year' in numeric_df.columns else [target_col])\n",
    "    y = numeric_df[target_col]\n",
    "    \n",
    "    # Remove any remaining NaN values\n",
    "    mask = ~(X.isna().any(axis=1) | y.isna())\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"❌ No valid samples for feature importance analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"🎯 Feature Importance Analysis for: {target_col}\")\n",
    "    print(f\"📊 Features: {X.shape[1]}, Samples: {X.shape[0]}\")\n",
    "    \n",
    "    importance_results = {}\n",
    "    \n",
    "    # Method 1: Correlation-based importance\n",
    "    correlations = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "    importance_results['correlation'] = correlations\n",
    "    \n",
    "    # Method 2: Mutual Information (if enough samples)\n",
    "    if len(X) > 5:\n",
    "        try:\n",
    "            mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "            mi_importance = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
    "            importance_results['mutual_info'] = mi_importance\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Mutual information calculation failed: {e}\")\n",
    "    \n",
    "    # Method 3: F-statistic\n",
    "    try:\n",
    "        f_scores, p_values = f_regression(X, y)\n",
    "        f_importance = pd.Series(f_scores, index=X.columns).sort_values(ascending=False)\n",
    "        importance_results['f_statistic'] = f_importance\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ F-statistic calculation failed: {e}\")\n",
    "    \n",
    "    return importance_results, X, y\n",
    "\n",
    "# Analyze feature importance\n",
    "importance_results, X_features, y_target = analyze_feature_importance(market_with_domain)\n",
    "\n",
    "if importance_results:\n",
    "    print(\"\\n🏆 TOP 10 FEATURES BY CORRELATION:\")\n",
    "    print(\"=\" * 50)\n",
    "    top_corr = importance_results['correlation'].head(10)\n",
    "    for i, (feature, score) in enumerate(top_corr.items(), 1):\n",
    "        print(f\"{i:2d}. {feature}: {score:.3f}\")\n",
    "    \n",
    "    if 'mutual_info' in importance_results:\n",
    "        print(\"\\n🧠 TOP 10 FEATURES BY MUTUAL INFORMATION:\")\n",
    "        print(\"=\" * 50)\n",
    "        top_mi = importance_results['mutual_info'].head(10)\n",
    "        for i, (feature, score) in enumerate(top_mi.items(), 1):\n",
    "            print(f\"{i:2d}. {feature}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e879827c",
   "metadata": {},
   "source": [
    "## 10. Feature Selection and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d70c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top features based on multiple criteria\n",
    "def select_top_features(importance_results, X, n_features=20):\n",
    "    \"\"\"\n",
    "    Select top features using ensemble of methods\n",
    "    \"\"\"\n",
    "    feature_scores = pd.DataFrame(index=X.columns)\n",
    "    \n",
    "    # Add scores from different methods\n",
    "    if 'correlation' in importance_results:\n",
    "        feature_scores['correlation'] = importance_results['correlation']\n",
    "    \n",
    "    if 'mutual_info' in importance_results:\n",
    "        # Normalize mutual info scores\n",
    "        mi_normalized = importance_results['mutual_info'] / importance_results['mutual_info'].max()\n",
    "        feature_scores['mutual_info'] = mi_normalized\n",
    "    \n",
    "    if 'f_statistic' in importance_results:\n",
    "        # Normalize F-statistic scores\n",
    "        f_normalized = importance_results['f_statistic'] / importance_results['f_statistic'].max()\n",
    "        feature_scores['f_statistic'] = f_normalized\n",
    "    \n",
    "    # Calculate ensemble score (average of available methods)\n",
    "    feature_scores['ensemble_score'] = feature_scores.mean(axis=1, skipna=True)\n",
    "    \n",
    "    # Select top features\n",
    "    top_features = feature_scores.nlargest(n_features, 'ensemble_score')\n",
    "    \n",
    "    return top_features, feature_scores\n",
    "\n",
    "# Select top features\n",
    "if importance_results and X_features is not None:\n",
    "    top_features, all_scores = select_top_features(importance_results, X_features, n_features=15)\n",
    "    \n",
    "    print(\"🌟 TOP 15 SELECTED FEATURES:\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, (feature, row) in enumerate(top_features.iterrows(), 1):\n",
    "        score = row['ensemble_score']\n",
    "        print(f\"{i:2d}. {feature}: {score:.3f}\")\n",
    "    \n",
    "    # Create feature selection visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_15 = top_features.head(15)\n",
    "    plt.barh(range(len(top_15)), top_15['ensemble_score'])\n",
    "    plt.yticks(range(len(top_15)), [f.replace('_', ' ').title() for f in top_15.index])\n",
    "    plt.xlabel('Ensemble Importance Score')\n",
    "    plt.title('Top 15 Features by Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save selected features list\n",
    "    selected_features = top_features.index.tolist()\n",
    "else:\n",
    "    print(\"⚠️ Feature importance analysis not available\")\n",
    "    selected_features = [col for col in X_features.columns if col != 'year'][:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd1b9c",
   "metadata": {},
   "source": [
    "## 11. Feature Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for machine learning\n",
    "def prepare_ml_features(df, selected_features):\n",
    "    \"\"\"\n",
    "    Prepare features for machine learning models\n",
    "    \"\"\"\n",
    "    # Create feature matrix\n",
    "    feature_df = df[selected_features + ['year']].copy()  # Include year for reference\n",
    "    \n",
    "    # Separate scaling for different feature types\n",
    "    numeric_features = feature_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'year' in numeric_features:\n",
    "        numeric_features.remove('year')\n",
    "    \n",
    "    # Standard scaling\n",
    "    scaler_standard = StandardScaler()\n",
    "    feature_df_scaled = feature_df.copy()\n",
    "    \n",
    "    if numeric_features:\n",
    "        # Handle NaN values before scaling\n",
    "        feature_subset = feature_df[numeric_features].fillna(feature_df[numeric_features].mean())\n",
    "        feature_df_scaled[numeric_features] = scaler_standard.fit_transform(feature_subset)\n",
    "    \n",
    "    # Min-Max scaling (alternative)\n",
    "    scaler_minmax = MinMaxScaler()\n",
    "    feature_df_minmax = feature_df.copy()\n",
    "    \n",
    "    if numeric_features:\n",
    "        feature_subset = feature_df[numeric_features].fillna(feature_df[numeric_features].mean())\n",
    "        feature_df_minmax[numeric_features] = scaler_minmax.fit_transform(feature_subset)\n",
    "    \n",
    "    return feature_df_scaled, feature_df_minmax, scaler_standard, scaler_minmax\n",
    "\n",
    "# Prepare scaled features\n",
    "if 'selected_features' in locals():\n",
    "    features_scaled, features_minmax, std_scaler, mm_scaler = prepare_ml_features(\n",
    "        market_with_domain, selected_features\n",
    "    )\n",
    "    \n",
    "    print(\"⚖️ FEATURE SCALING COMPLETED\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"✅ Standard scaled features shape: {features_scaled.shape}\")\n",
    "    print(f\"✅ Min-Max scaled features shape: {features_minmax.shape}\")\n",
    "    print(f\"✅ Selected features: {len(selected_features)}\")\n",
    "    \n",
    "    # Show scaling example for first feature\n",
    "    if selected_features:\n",
    "        first_feature = selected_features[0]\n",
    "        print(f\"\\n📊 Scaling Example - {first_feature}:\")\n",
    "        print(f\"   Original: {market_with_domain[first_feature].describe()['mean']:.2f} ± {market_with_domain[first_feature].describe()['std']:.2f}\")\n",
    "        print(f\"   Std Scaled: {features_scaled[first_feature].mean():.2f} ± {features_scaled[first_feature].std():.2f}\")\n",
    "        print(f\"   MinMax Scaled: [{features_minmax[first_feature].min():.2f}, {features_minmax[first_feature].max():.2f}]\")\n",
    "else:\n",
    "    print(\"⚠️ No features selected for scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c30498",
   "metadata": {},
   "source": [
    "## 12. Feature Engineering Summary and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b156ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total features created\n",
    "original_features = len(market_df.columns)\n",
    "total_features = len(market_with_domain.columns)\n",
    "new_features = total_features - original_features\n",
    "\n",
    "print(\"🏗️ FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"📊 Original features: {original_features}\")\n",
    "print(f\"📊 Total features after engineering: {total_features}\")\n",
    "print(f\"📊 New features created: {new_features}\")\n",
    "print(f\"📊 Feature expansion ratio: {total_features/original_features:.1f}x\")\n",
    "\n",
    "print(\"\\n✅ FEATURE CATEGORIES CREATED:\")\n",
    "feature_categories = {\n",
    "    'Growth Features': len([c for c in market_with_domain.columns if 'growth' in c]),\n",
    "    'Trend Features': len([c for c in market_with_domain.columns if any(x in c for x in ['_ma_', '_std_', 'trend'])]),\n",
    "    'Lag Features': len([c for c in market_with_domain.columns if 'lag' in c]),\n",
    "    'Interaction Features': len([c for c in market_with_domain.columns if any(x in c for x in ['ratio', 'efficiency'])]),\n",
    "    'Technical Indicators': len([c for c in market_with_domain.columns if any(x in c for x in ['momentum', 'volatility'])]),\n",
    "    'Temporal Features': len([c for c in market_with_domain.columns if any(x in c for x in ['cycle', 'period', 'sin'])]),\n",
    "    'Domain Features': len([c for c in market_with_domain.columns if any(x in c for x in ['stage', 'attractiveness'])])\n",
    "}\n",
    "\n",
    "for category, count in feature_categories.items():\n",
    "    print(f\"   📈 {category}: {count} features\")\n",
    "\n",
    "# Save engineered features\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save main engineered dataset\n",
    "market_with_domain.to_csv(output_dir / 'ai_market_engineered.csv', index=False)\n",
    "\n",
    "# Save scaled versions\n",
    "if 'features_scaled' in locals():\n",
    "    features_scaled.to_csv(output_dir / 'ai_market_features_standard_scaled.csv', index=False)\n",
    "    features_minmax.to_csv(output_dir / 'ai_market_features_minmax_scaled.csv', index=False)\n",
    "\n",
    "# Save feature lists\n",
    "feature_info = {\n",
    "    'all_features': list(market_with_domain.columns),\n",
    "    'selected_features': selected_features if 'selected_features' in locals() else [],\n",
    "    'feature_categories': feature_categories,\n",
    "    'engineering_timestamp': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "feature_list_df = pd.DataFrame({\n",
    "    'feature_name': market_with_domain.columns,\n",
    "    'feature_type': ['engineered' if col not in market_df.columns else 'original' \n",
    "                    for col in market_with_domain.columns]\n",
    "})\n",
    "feature_list_df.to_csv(output_dir / 'feature_list.csv', index=False)\n",
    "\n",
    "print(\"\\n💾 SAVED FILES:\")\n",
    "print(f\"   📄 ai_market_engineered.csv ({market_with_domain.shape})\")\n",
    "if 'features_scaled' in locals():\n",
    "    print(f\"   📄 ai_market_features_standard_scaled.csv ({features_scaled.shape})\")\n",
    "    print(f\"   📄 ai_market_features_minmax_scaled.csv ({features_minmax.shape})\")\n",
    "print(f\"   📄 feature_list.csv ({len(feature_list_df)} features)\")\n",
    "\n",
    "print(\"\\n🚀 NEXT STEPS:\")\n",
    "print(\"   1. Move to 04_eda_visualization.ipynb for detailed analysis\")\n",
    "print(\"   2. Use engineered features in 05_model_development.ipynb\")\n",
    "print(\"   3. Focus on top selected features for modeling\")\n",
    "\n",
    "print(\"\\n✅ Feature Engineering Phase Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
