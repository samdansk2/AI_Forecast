{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0ef336",
   "metadata": {},
   "source": [
    "# Model Development & Training - AI/ML Market Analysis\n",
    "\n",
    "This notebook implements multiple machine learning models to predict AI market trends and analyze patterns.\n",
    "\n",
    "## Objectives:\n",
    "- Implement time series forecasting models (ARIMA/Prophet)\n",
    "- Build machine learning models (XGBoost, Random Forest)\n",
    "- Perform cross-validation and hyperparameter tuning\n",
    "- Compare model performance and create ensembles\n",
    "- Generate model evaluation metrics and diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01734e5f",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f962f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comprehensive ML libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# Time Series\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Prophet not available, will use alternative time series methods\")\n",
    "    PROPHET_AVAILABLE = False\n",
    "\n",
    "# Utilities\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Load engineered data\n",
    "processed_dir = Path('../data/processed')\n",
    "market_df = pd.read_csv(processed_dir / 'ai_market_engineered.csv')\n",
    "features_scaled = pd.read_csv(processed_dir / 'ai_market_features_standard_scaled.csv')\n",
    "\n",
    "print(\"‚úÖ Data and libraries loaded successfully!\")\n",
    "print(f\"Market data: {market_df.shape}\")\n",
    "print(f\"Scaled features: {features_scaled.shape}\")\n",
    "print(f\"Prophet available: {PROPHET_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda3a8b",
   "metadata": {},
   "source": [
    "## 2. Model Preparation and Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "def prepare_modeling_data(df, target_col='ai_software_revenue_in_billions'):\n",
    "    \"\"\"\n",
    "    Prepare data for machine learning models\n",
    "    \"\"\"\n",
    "    # Clean data\n",
    "    model_df = df.dropna(subset=[target_col])\n",
    "    \n",
    "    # Select features (exclude target and non-predictive columns)\n",
    "    exclude_cols = [target_col, 'year'] + [col for col in df.columns if target_col in col and col != target_col]\n",
    "    feature_cols = [col for col in model_df.columns if col not in exclude_cols and model_df[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    X = model_df[feature_cols].fillna(model_df[feature_cols].mean())\n",
    "    y = model_df[target_col]\n",
    "    \n",
    "    print(f\"üéØ Target Variable: {target_col}\")\n",
    "    print(f\"üìä Features: {len(feature_cols)}\")\n",
    "    print(f\"üìà Samples: {len(X)}\")\n",
    "    print(f\"üìã Feature columns: {feature_cols[:5]}...\" if len(feature_cols) > 5 else f\"üìã Feature columns: {feature_cols}\")\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "# Prepare data for multiple targets\n",
    "targets = {\n",
    "    'ai_software_revenue_in_billions': 'AI Software Revenue',\n",
    "    'global_ai_market_value_in_billions': 'AI Market Value',\n",
    "    'ai_adoption': 'AI Adoption Rate'\n",
    "}\n",
    "\n",
    "modeling_data = {}\n",
    "\n",
    "for target_col, target_name in targets.items():\n",
    "    if target_col in market_df.columns:\n",
    "        print(f\"\\nüîß Preparing data for {target_name}...\")\n",
    "        X, y, feature_cols = prepare_modeling_data(market_df, target_col)\n",
    "        \n",
    "        if len(X) > 0:\n",
    "            modeling_data[target_col] = {\n",
    "                'X': X,\n",
    "                'y': y,\n",
    "                'features': feature_cols,\n",
    "                'name': target_name\n",
    "            }\n",
    "        else:\n",
    "            print(f\"‚ùå Insufficient data for {target_name}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Prepared data for {len(modeling_data)} target variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b544b",
   "metadata": {},
   "source": [
    "## 3. Time Series Forecasting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5994bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series forecasting\n",
    "def build_time_series_models(df, target_col, year_col='year', forecast_periods=3):\n",
    "    \"\"\"\n",
    "    Build various time series forecasting models\n",
    "    \"\"\"\n",
    "    if target_col not in df.columns or year_col not in df.columns:\n",
    "        return None\n",
    "    \n",
    "    # Prepare time series data\n",
    "    ts_data = df[[year_col, target_col]].dropna()\n",
    "    ts_data = ts_data.sort_values(year_col)\n",
    "    \n",
    "    if len(ts_data) < 5:\n",
    "        print(f\"‚ö†Ô∏è Insufficient data for time series modeling: {len(ts_data)} points\")\n",
    "        return None\n",
    "    \n",
    "    models = {}\n",
    "    forecasts = {}\n",
    "    \n",
    "    print(f\"‚è∞ Building time series models for {target_col}...\")\n",
    "    \n",
    "    # 1. Linear Trend Model\n",
    "    try:\n",
    "        X_trend = ts_data[year_col].values.reshape(-1, 1)\n",
    "        y_trend = ts_data[target_col].values\n",
    "        \n",
    "        linear_model = LinearRegression()\n",
    "        linear_model.fit(X_trend, y_trend)\n",
    "        \n",
    "        # Forecast\n",
    "        future_years = np.arange(ts_data[year_col].max() + 1, \n",
    "                                ts_data[year_col].max() + forecast_periods + 1).reshape(-1, 1)\n",
    "        linear_forecast = linear_model.predict(future_years)\n",
    "        \n",
    "        models['linear_trend'] = linear_model\n",
    "        forecasts['linear_trend'] = {\n",
    "            'years': future_years.flatten(),\n",
    "            'values': linear_forecast\n",
    "        }\n",
    "        \n",
    "        # Calculate R¬≤\n",
    "        r2 = linear_model.score(X_trend, y_trend)\n",
    "        print(f\"   ‚úÖ Linear Trend Model: R¬≤ = {r2:.3f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Linear trend model failed: {e}\")\n",
    "    \n",
    "    # 2. Exponential Smoothing\n",
    "    try:\n",
    "        exp_model = ExponentialSmoothing(\n",
    "            ts_data[target_col].values,\n",
    "            trend='add',\n",
    "            seasonal=None,\n",
    "            initialization_method='estimated'\n",
    "        ).fit()\n",
    "        \n",
    "        exp_forecast = exp_model.forecast(forecast_periods)\n",
    "        \n",
    "        models['exponential_smoothing'] = exp_model\n",
    "        forecasts['exponential_smoothing'] = {\n",
    "            'years': np.arange(ts_data[year_col].max() + 1, \n",
    "                              ts_data[year_col].max() + forecast_periods + 1),\n",
    "            'values': exp_forecast\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úÖ Exponential Smoothing Model: AIC = {exp_model.aic:.1f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Exponential smoothing failed: {e}\")\n",
    "    \n",
    "    # 3. ARIMA Model\n",
    "    try:\n",
    "        # Auto-select ARIMA parameters (simple approach)\n",
    "        arima_model = ARIMA(ts_data[target_col].values, order=(1, 1, 1))\n",
    "        arima_fitted = arima_model.fit()\n",
    "        \n",
    "        arima_forecast = arima_fitted.forecast(steps=forecast_periods)\n",
    "        \n",
    "        models['arima'] = arima_fitted\n",
    "        forecasts['arima'] = {\n",
    "            'years': np.arange(ts_data[year_col].max() + 1, \n",
    "                              ts_data[year_col].max() + forecast_periods + 1),\n",
    "            'values': arima_forecast\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úÖ ARIMA Model: AIC = {arima_fitted.aic:.1f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ARIMA model failed: {e}\")\n",
    "    \n",
    "    # 4. Prophet Model (if available)\n",
    "    if PROPHET_AVAILABLE:\n",
    "        try:\n",
    "            # Prepare Prophet data format\n",
    "            prophet_df = pd.DataFrame({\n",
    "                'ds': pd.to_datetime(ts_data[year_col], format='%Y'),\n",
    "                'y': ts_data[target_col].values\n",
    "            })\n",
    "            \n",
    "            prophet_model = Prophet(\n",
    "                yearly_seasonality=False,\n",
    "                weekly_seasonality=False,\n",
    "                daily_seasonality=False\n",
    "            )\n",
    "            prophet_model.fit(prophet_df)\n",
    "            \n",
    "            # Create future dataframe\n",
    "            future = prophet_model.make_future_dataframe(periods=forecast_periods, freq='Y')\n",
    "            prophet_forecast = prophet_model.predict(future)\n",
    "            \n",
    "            models['prophet'] = prophet_model\n",
    "            forecasts['prophet'] = {\n",
    "                'years': np.arange(ts_data[year_col].max() + 1, \n",
    "                                  ts_data[year_col].max() + forecast_periods + 1),\n",
    "                'values': prophet_forecast['yhat'].tail(forecast_periods).values\n",
    "            }\n",
    "            \n",
    "            print(f\"   ‚úÖ Prophet Model: Successfully trained\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Prophet model failed: {e}\")\n",
    "    \n",
    "    return models, forecasts, ts_data\n",
    "\n",
    "# Build time series models for key metrics\n",
    "ts_models = {}\n",
    "ts_forecasts = {}\n",
    "\n",
    "for target_col, target_name in targets.items():\n",
    "    if target_col in market_df.columns:\n",
    "        print(f\"\\nüïí Time Series Modeling for {target_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        models, forecasts, ts_data = build_time_series_models(market_df, target_col)\n",
    "        \n",
    "        if models:\n",
    "            ts_models[target_col] = models\n",
    "            ts_forecasts[target_col] = forecasts\n",
    "\n",
    "print(f\"\\n‚úÖ Time series models built for {len(ts_models)} targets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e55607",
   "metadata": {},
   "source": [
    "## 4. Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build machine learning models\n",
    "def build_ml_models(X, y, test_size=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Build and evaluate multiple ML models\n",
    "    \"\"\"\n",
    "    if len(X) < 5:\n",
    "        print(\"‚ùå Insufficient data for ML modeling\")\n",
    "        return None, None\n",
    "    \n",
    "    # Split data (time series aware)\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"üìä Data split: Train={len(X_train)}, Test={len(X_test)}\")\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        'linear_regression': LinearRegression(),\n",
    "        'ridge_regression': Ridge(alpha=1.0, random_state=random_state),\n",
    "        'lasso_regression': Lasso(alpha=0.1, random_state=random_state),\n",
    "        'random_forest': RandomForestRegressor(n_estimators=100, random_state=random_state),\n",
    "        'gradient_boosting': GradientBoostingRegressor(n_estimators=100, random_state=random_state),\n",
    "        'xgboost': xgb.XGBRegressor(n_estimators=100, random_state=random_state)\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    results = {}\n",
    "    trained_models = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            # Train model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test) if len(X_test) > 0 else np.array([])\n",
    "            \n",
    "            # Metrics\n",
    "            train_r2 = r2_score(y_train, y_train_pred)\n",
    "            train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "            train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "            \n",
    "            result = {\n",
    "                'train_r2': train_r2,\n",
    "                'train_rmse': train_rmse,\n",
    "                'train_mae': train_mae,\n",
    "                'train_pred': y_train_pred,\n",
    "                'test_pred': y_test_pred\n",
    "            }\n",
    "            \n",
    "            if len(X_test) > 0:\n",
    "                test_r2 = r2_score(y_test, y_test_pred)\n",
    "                test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "                test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "                \n",
    "                result.update({\n",
    "                    'test_r2': test_r2,\n",
    "                    'test_rmse': test_rmse,\n",
    "                    'test_mae': test_mae\n",
    "                })\n",
    "            \n",
    "            results[name] = result\n",
    "            trained_models[name] = model\n",
    "            \n",
    "            print(f\"   ‚úÖ {name.replace('_', ' ').title()}: R¬≤ = {train_r2:.3f}, RMSE = {train_rmse:.2f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå {name} failed: {e}\")\n",
    "    \n",
    "    return results, trained_models\n",
    "\n",
    "# Build ML models for each target\n",
    "ml_results = {}\n",
    "ml_models = {}\n",
    "\n",
    "for target_col, data in modeling_data.items():\n",
    "    print(f\"\\nü§ñ Building ML Models for {data['name']}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    results, models = build_ml_models(data['X'], data['y'])\n",
    "    \n",
    "    if results:\n",
    "        ml_results[target_col] = results\n",
    "        ml_models[target_col] = models\n",
    "\n",
    "print(f\"\\n‚úÖ ML models built for {len(ml_results)} targets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ce057",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "def compare_model_performance(results_dict, target_name):\n",
    "    \"\"\"\n",
    "    Create comprehensive model comparison\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä MODEL PERFORMANCE COMPARISON - {target_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, metrics in results_dict.items():\n",
    "        row = {\n",
    "            'Model': model_name.replace('_', ' ').title(),\n",
    "            'Train_R2': metrics.get('train_r2', np.nan),\n",
    "            'Train_RMSE': metrics.get('train_rmse', np.nan),\n",
    "            'Train_MAE': metrics.get('train_mae', np.nan),\n",
    "            'Test_R2': metrics.get('test_r2', np.nan),\n",
    "            'Test_RMSE': metrics.get('test_rmse', np.nan),\n",
    "            'Test_MAE': metrics.get('test_mae', np.nan)\n",
    "        }\n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Display results\n",
    "    pd.set_option('display.precision', 3)\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Find best model\n",
    "    if 'Train_R2' in comparison_df.columns:\n",
    "        best_model_idx = comparison_df['Train_R2'].idxmax()\n",
    "        best_model = comparison_df.loc[best_model_idx, 'Model']\n",
    "        best_r2 = comparison_df.loc[best_model_idx, 'Train_R2']\n",
    "        \n",
    "        print(f\"\\nüèÜ Best Model: {best_model} (R¬≤ = {best_r2:.3f})\")\n",
    "    \n",
    "    # Create visualization\n",
    "    if len(comparison_df) > 1:\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            x=comparison_df['Model'],\n",
    "            y=comparison_df['Train_R2'],\n",
    "            name='Training R¬≤',\n",
    "            marker_color='lightblue'\n",
    "        ))\n",
    "        \n",
    "        if 'Test_R2' in comparison_df.columns and not comparison_df['Test_R2'].isna().all():\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=comparison_df['Model'],\n",
    "                y=comparison_df['Test_R2'],\n",
    "                name='Testing R¬≤',\n",
    "                marker_color='lightcoral'\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'Model Performance Comparison - {target_name}',\n",
    "            xaxis_title='Model',\n",
    "            yaxis_title='R¬≤ Score',\n",
    "            height=500\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Compare models for each target\n",
    "model_comparisons = {}\n",
    "\n",
    "for target_col, results in ml_results.items():\n",
    "    target_name = targets.get(target_col, target_col)\n",
    "    comparison = compare_model_performance(results, target_name)\n",
    "    model_comparisons[target_col] = comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28aee3",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c802a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance from tree-based models\n",
    "def analyze_feature_importance(models_dict, feature_names, target_name):\n",
    "    \"\"\"\n",
    "    Extract and visualize feature importance\n",
    "    \"\"\"\n",
    "    importance_data = {}\n",
    "    \n",
    "    # Extract importance from tree-based models\n",
    "    tree_models = ['random_forest', 'gradient_boosting', 'xgboost']\n",
    "    \n",
    "    for model_name in tree_models:\n",
    "        if model_name in models_dict:\n",
    "            model = models_dict[model_name]\n",
    "            \n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importance_data[model_name] = model.feature_importances_\n",
    "    \n",
    "    if importance_data:\n",
    "        # Create importance DataFrame\n",
    "        importance_df = pd.DataFrame(importance_data, index=feature_names)\n",
    "        \n",
    "        # Calculate average importance\n",
    "        importance_df['average'] = importance_df.mean(axis=1)\n",
    "        importance_df = importance_df.sort_values('average', ascending=False)\n",
    "        \n",
    "        print(f\"\\nüéØ FEATURE IMPORTANCE - {target_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Show top 10 features\n",
    "        top_features = importance_df.head(10)\n",
    "        for i, (feature, row) in enumerate(top_features.iterrows(), 1):\n",
    "            avg_importance = row['average']\n",
    "            feature_display = feature.replace('_', ' ').title()[:40]\n",
    "            print(f\"{i:2d}. {feature_display}: {avg_importance:.3f}\")\n",
    "        \n",
    "        # Create visualization\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        colors = px.colors.qualitative.Set3\n",
    "        for i, model_name in enumerate(importance_data.keys()):\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=top_features.index,\n",
    "                y=top_features[model_name],\n",
    "                name=model_name.replace('_', ' ').title(),\n",
    "                marker_color=colors[i % len(colors)]\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'Feature Importance Comparison - {target_name}',\n",
    "            xaxis_title='Features',\n",
    "            yaxis_title='Importance Score',\n",
    "            height=600,\n",
    "            xaxis_tickangle=-45\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        return importance_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Analyze feature importance for each target\n",
    "feature_importance_results = {}\n",
    "\n",
    "for target_col, models in ml_models.items():\n",
    "    if target_col in modeling_data:\n",
    "        target_name = modeling_data[target_col]['name']\n",
    "        feature_names = modeling_data[target_col]['features']\n",
    "        \n",
    "        importance_df = analyze_feature_importance(models, feature_names, target_name)\n",
    "        if importance_df is not None:\n",
    "            feature_importance_results[target_col] = importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a408380",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation and Model Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1810514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "def perform_cross_validation(X, y, models_dict, cv_folds=3):\n",
    "    \"\"\"\n",
    "    Perform time series cross-validation\n",
    "    \"\"\"\n",
    "    if len(X) < cv_folds + 2:\n",
    "        print(f\"‚ö†Ô∏è Insufficient data for {cv_folds}-fold CV: {len(X)} samples\")\n",
    "        return None\n",
    "    \n",
    "    # Use TimeSeriesSplit for time series data\n",
    "    tscv = TimeSeriesSplit(n_splits=cv_folds)\n",
    "    \n",
    "    cv_results = {}\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        try:\n",
    "            # Perform cross-validation\n",
    "            scores = cross_val_score(model, X, y, cv=tscv, scoring='r2')\n",
    "            \n",
    "            cv_results[name] = {\n",
    "                'scores': scores,\n",
    "                'mean': scores.mean(),\n",
    "                'std': scores.std(),\n",
    "                'min': scores.min(),\n",
    "                'max': scores.max()\n",
    "            }\n",
    "            \n",
    "            print(f\"üìä {name.replace('_', ' ').title()}:\")\n",
    "            print(f\"   CV R¬≤ = {scores.mean():.3f} ¬± {scores.std():.3f}\")\n",
    "            print(f\"   Range: [{scores.min():.3f}, {scores.max():.3f}]\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå CV failed for {name}: {e}\")\n",
    "    \n",
    "    return cv_results\n",
    "\n",
    "# Perform cross-validation for each target\n",
    "cv_results_all = {}\n",
    "\n",
    "for target_col, models in ml_models.items():\n",
    "    if target_col in modeling_data:\n",
    "        target_name = modeling_data[target_col]['name']\n",
    "        X = modeling_data[target_col]['X']\n",
    "        y = modeling_data[target_col]['y']\n",
    "        \n",
    "        print(f\"\\nüîÑ Cross-Validation for {target_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        cv_results = perform_cross_validation(X, y, models)\n",
    "        if cv_results:\n",
    "            cv_results_all[target_col] = cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4793bd95",
   "metadata": {},
   "source": [
    "## 8. Model Ensemble and Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35192d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble models\n",
    "def create_ensemble_model(models_dict, X, y):\n",
    "    \"\"\"\n",
    "    Create simple ensemble using best performing models\n",
    "    \"\"\"\n",
    "    # Get predictions from all models\n",
    "    predictions = []\n",
    "    model_names = []\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        try:\n",
    "            pred = model.predict(X)\n",
    "            predictions.append(pred)\n",
    "            model_names.append(name)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to get predictions from {name}: {e}\")\n",
    "    \n",
    "    if len(predictions) >= 2:\n",
    "        # Simple average ensemble\n",
    "        ensemble_pred = np.mean(predictions, axis=0)\n",
    "        \n",
    "        # Weighted ensemble (weight by R¬≤ score)\n",
    "        weights = []\n",
    "        for name in model_names:\n",
    "            try:\n",
    "                pred = model.predict(X)\n",
    "                r2 = r2_score(y, pred)\n",
    "                weights.append(max(0, r2))  # Use 0 for negative R¬≤\n",
    "            except:\n",
    "                weights.append(0)\n",
    "        \n",
    "        if sum(weights) > 0:\n",
    "            weights = np.array(weights) / sum(weights)\n",
    "            weighted_ensemble_pred = np.average(predictions, axis=0, weights=weights)\n",
    "        else:\n",
    "            weighted_ensemble_pred = ensemble_pred\n",
    "        \n",
    "        # Evaluate ensemble\n",
    "        simple_r2 = r2_score(y, ensemble_pred)\n",
    "        weighted_r2 = r2_score(y, weighted_ensemble_pred)\n",
    "        \n",
    "        print(f\"üîó Ensemble Results:\")\n",
    "        print(f\"   Simple Average: R¬≤ = {simple_r2:.3f}\")\n",
    "        print(f\"   Weighted Average: R¬≤ = {weighted_r2:.3f}\")\n",
    "        print(f\"   Models in ensemble: {len(model_names)}\")\n",
    "        \n",
    "        return {\n",
    "            'simple_ensemble': ensemble_pred,\n",
    "            'weighted_ensemble': weighted_ensemble_pred,\n",
    "            'simple_r2': simple_r2,\n",
    "            'weighted_r2': weighted_r2,\n",
    "            'model_names': model_names,\n",
    "            'weights': weights\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Create ensembles for each target\n",
    "ensemble_results = {}\n",
    "\n",
    "for target_col, models in ml_models.items():\n",
    "    if target_col in modeling_data:\n",
    "        target_name = modeling_data[target_col]['name']\n",
    "        X = modeling_data[target_col]['X']\n",
    "        y = modeling_data[target_col]['y']\n",
    "        \n",
    "        print(f\"\\nüîó Creating Ensemble for {target_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        ensemble = create_ensemble_model(models, X, y)\n",
    "        if ensemble:\n",
    "            ensemble_results[target_col] = ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858daeef",
   "metadata": {},
   "source": [
    "## 9. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c55ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models and results\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "results_dir = Path('../results')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"üíæ SAVING MODELS AND RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Save ML models\n",
    "for target_col, models in ml_models.items():\n",
    "    target_dir = models_dir / target_col.replace('_', '')\n",
    "    target_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        model_path = target_dir / f'{model_name}.joblib'\n",
    "        joblib.dump(model, model_path)\n",
    "        print(f\"   ‚úÖ Saved {model_name} for {target_col}\")\n",
    "\n",
    "# Save model comparison results\n",
    "for target_col, comparison in model_comparisons.items():\n",
    "    comparison_path = results_dir / f'model_comparison_{target_col}.csv'\n",
    "    comparison.to_csv(comparison_path, index=False)\n",
    "    print(f\"   üìä Saved comparison for {target_col}\")\n",
    "\n",
    "# Save cross-validation results\n",
    "cv_summary = []\n",
    "for target_col, cv_data in cv_results_all.items():\n",
    "    for model_name, scores in cv_data.items():\n",
    "        cv_summary.append({\n",
    "            'target': target_col,\n",
    "            'model': model_name,\n",
    "            'cv_mean': scores['mean'],\n",
    "            'cv_std': scores['std'],\n",
    "            'cv_min': scores['min'],\n",
    "            'cv_max': scores['max']\n",
    "        })\n",
    "\n",
    "if cv_summary:\n",
    "    cv_df = pd.DataFrame(cv_summary)\n",
    "    cv_df.to_csv(results_dir / 'cross_validation_results.csv', index=False)\n",
    "    print(f\"   üìä Saved cross-validation results\")\n",
    "\n",
    "# Save ensemble results\n",
    "ensemble_summary = []\n",
    "for target_col, ensemble in ensemble_results.items():\n",
    "    ensemble_summary.append({\n",
    "        'target': target_col,\n",
    "        'simple_ensemble_r2': ensemble['simple_r2'],\n",
    "        'weighted_ensemble_r2': ensemble['weighted_r2'],\n",
    "        'models_count': len(ensemble['model_names']),\n",
    "        'models_used': ', '.join(ensemble['model_names'])\n",
    "    })\n",
    "\n",
    "if ensemble_summary:\n",
    "    ensemble_df = pd.DataFrame(ensemble_summary)\n",
    "    ensemble_df.to_csv(results_dir / 'ensemble_results.csv', index=False)\n",
    "    print(f\"   üîó Saved ensemble results\")\n",
    "\n",
    "# Create model development summary\n",
    "print(f\"\\nüìã MODEL DEVELOPMENT SUMMARY:\")\n",
    "print(f\"   ‚úÖ Targets modeled: {len(ml_models)}\")\n",
    "print(f\"   ‚úÖ Total models trained: {sum(len(models) for models in ml_models.values())}\")\n",
    "print(f\"   ‚úÖ Time series models: {len(ts_models)}\")\n",
    "print(f\"   ‚úÖ Ensemble models: {len(ensemble_results)}\")\n",
    "print(f\"   ‚úÖ Cross-validation completed: {len(cv_results_all)} targets\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"   1. Move to 06_predictions.ipynb for forecasting\")\n",
    "print(\"   2. Use best performing models for future predictions\")\n",
    "print(\"   3. Generate confidence intervals and scenarios\")\n",
    "\n",
    "print(\"\\n‚úÖ Model Development Phase Complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
